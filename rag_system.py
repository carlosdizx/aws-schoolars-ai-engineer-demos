import boto3
import json
import chromadb
from chromadb import Documents, EmbeddingFunction, Embeddings

# Initialize Bedrock client
bedrock_runtime = boto3.client('bedrock-runtime', region_name='us-east-1')

# Model configuration
EMBEDDING_MODEL = "amazon.titan-embed-text-v1"
TEXT_GENERATION_MODEL = "anthropic.claude-3-haiku-20240307-v1:0"  # Claude 3 Haiku Model


class BedrockEmbeddingFunction(EmbeddingFunction):
    """
    Custom embedding function for Amazon Bedrock
    Compatible with ChromaDB
    """
    
    def __init__(self):
        pass
    
    def __call__(self, input: Documents) -> Embeddings:
        """
        Gets embeddings for a list of texts
        
        Args:
            input: List of texts to convert to embeddings
            
        Returns:
            List of embeddings (each one is a list of floats)
        """
        embeddings = []
        for text in input:
            body = json.dumps({"inputText": text})
            try:
                response = bedrock_runtime.invoke_model(
                    modelId=EMBEDDING_MODEL,
                    body=body,
                    contentType='application/json',
                    accept='application/json'
                )
                response_body = json.loads(response['body'].read())
                embeddings.append(response_body['embedding'])
            except Exception as e:
                print(f"Error getting embedding: {e}")
                raise
        return embeddings


def generate_text(prompt):
    """
    Generates text using Claude 3 on Amazon Bedrock
    
    Args:
        prompt: The prompt to generate text
        
    Returns:
        The text generated by the model
    """
    body = json.dumps({
        "anthropic_version": "bedrock-2023-05-31",
        "max_tokens": 500,
        "messages": [
            {
                "role": "user",
                "content": prompt
            }
        ],
        "temperature": 0.7,
        "top_p": 0.9,
    })
    
    try:
        response = bedrock_runtime.invoke_model(
            modelId=TEXT_GENERATION_MODEL, 
            body=body,
            contentType='application/json',
            accept='application/json'
        )
        response_body = json.loads(response['body'].read())
        return response_body['content'][0]['text']
    except Exception as e:
        print(f"Error generating text: {e}")
        raise


# Initialize Chroma client
chroma_client = chromadb.Client()

# Create custom embedding function using Bedrock
bedrock_ef = BedrockEmbeddingFunction()

# Create collection with custom embedding function
try:
    # Try to delete collection if it exists (to avoid errors on multiple runs)
    try:
        chroma_client.delete_collection(name="bedrock_docs")
    except:
        pass
    
    collection = chroma_client.create_collection(
        name="bedrock_docs",
        embedding_function=bedrock_ef
    )
    print("[OK] Chroma collection created successfully")
except Exception as e:
    print(f"Error creating collection: {e}")
    raise


def add_documents(docs):
    """
    Adds documents to the Chroma collection
    
    Args:
        docs: List of documents (strings) to index
    """
    try:
        collection.add(
            documents=docs,
            ids=[f"doc_{i}" for i in range(len(docs))]
        )
        print(f"[OK] {len(docs)} documents added to collection")
    except Exception as e:
        print(f"Error adding documents: {e}")
        raise


# Add sample documents
sample_docs = [
    "Amazon Bedrock is a fully managed service for foundation models.",
    "RAG systems combine retrieval and generation to improve responses.",
    "Embeddings are vector representations of text in high-dimensional spaces.",
    "Chroma is an efficient vector store for building AI applications.",
    "Foundation models can be fine-tuned for specific tasks and domains.",
    "Amazon Bedrock provides access to AI models from leading companies like Anthropic, AI21 Labs, and Amazon.",
    "RAG improves response accuracy by providing relevant context from stored knowledge.",
    "Embeddings enable searching for similar documents using cosine similarity.",
    "Claude is a language model developed by Anthropic available on Amazon Bedrock.",
    "RAG systems are especially useful for applications requiring domain-specific knowledge."
]

print("\nAdding sample documents...")
add_documents(sample_docs)


def rag_generate(query, top_k=2):
    """
    Generates a response using RAG (Retrieval-Augmented Generation)
    
    Args:
        query: The user's query
        top_k: Number of relevant documents to retrieve
        
    Returns:
        The generated response with context
    """
    try:
        # Retrieve relevant documents
        results = collection.query(
            query_texts=[query],
            n_results=top_k
        )
        
        # Build prompt with retrieved context
        context = "\n".join(results['documents'][0])
        
        prompt = f"""Given the following context, please answer the question.

Context: {context}

Question: {query}

Based on the provided context, my answer is:"""
        
        # Generate response
        response = generate_text(prompt)
        return response
    except Exception as e:
        print(f"Error in rag_generate: {e}")
        raise


def generate_without_rag(query):
    """
    Generates a response without using RAG (no additional context)
    
    Args:
        query: The user's query
        
    Returns:
        The generated response without context
    """
    try:
        prompt = query
        return generate_text(prompt)
    except Exception as e:
        print(f"Error in generate_without_rag: {e}")
        raise


def main():
    """Main function to test the RAG system"""
    print("\n" + "="*80)
    print("RAG SYSTEM WITH AMAZON BEDROCK")
    print("="*80 + "\n")
    
    # Test the RAG system with a query
    print("Initial RAG system test:\n")
    query = "How does Amazon Bedrock relate to RAG systems?"
    print(f"Query: {query}\n")
    
    response = rag_generate(query)
    print(f"RAG Response: {response}\n")
    
    # Compare responses with and without RAG
    print("\n" + "="*80)
    print("COMPARISON: RAG vs Without RAG")
    print("="*80 + "\n")
    
    test_queries = [
        "What are embeddings used for in AI?",
        "Explain the benefits of using RAG in AI applications.",
        "How does Amazon Bedrock support foundation models?"
    ]
    
    for query in test_queries:
        print(f"\n{'='*80}")
        print(f"Query: {query}")
        print(f"{'='*80}\n")
        
        print("[RAG] Response with RAG:")
        print("-" * 80)
        rag_response = rag_generate(query)
        print(rag_response)
        
        print("\n\n[WITHOUT RAG] Response without RAG:")
        print("-" * 80)
        no_rag_response = generate_without_rag(query)
        print(no_rag_response)
        
        print("\n" + "="*80)


if __name__ == "__main__":
    main()
